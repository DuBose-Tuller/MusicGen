{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d38f355-02b6-4626-80f3-86c5b3108371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/DAVIDSON/dutuller/Workspace/DRI1/MusicGen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAVIDSON/dutuller/Workspace/DRI1/MusicGen/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: embeddings/NHS/Dance/s15-t15/last_embeddings.h5\n",
      "\n",
      "Processed embeddings/NHS/Dance/s15-t15/last_embeddings.h5:\n",
      "  embeddings/NHS/Dance: 408 samples\n",
      "\n",
      "Processing file: embeddings/NHS/Love/s15-t15/last_embeddings.h5\n",
      "\n",
      "Processed embeddings/NHS/Love/s15-t15/last_embeddings.h5:\n",
      "  embeddings/NHS/Love: 365 samples\n",
      "\n",
      "Processing file: embeddings/NHS/Lullaby/s15-t15/last_embeddings.h5\n",
      "\n",
      "Processed embeddings/NHS/Lullaby/s15-t15/last_embeddings.h5:\n",
      "  embeddings/NHS/Lullaby: 158 samples\n",
      "\n",
      "Processing file: embeddings/NHS/Healing/s15-t15/last_embeddings.h5\n",
      "\n",
      "Processed embeddings/NHS/Healing/s15-t15/last_embeddings.h5:\n",
      "  embeddings/NHS/Healing: 527 samples\n",
      "\n",
      "Processing file: embeddings/NHS/samples/wav/last_embeddings.h5\n",
      "\n",
      "Processed embeddings/NHS/samples/wav/last_embeddings.h5:\n",
      "  embeddings/NHS/samples: 118 samples\n",
      "Processing fold 1/5\n",
      "Fold 1 - Generous Policy:\n",
      "  F1 Score: 0.1920\n",
      "  Confusion Matrix:\n",
      "[[1507 1723 1176 1181]\n",
      " [3602 3326 3589 2786]\n",
      " [1863 1242  954  339]\n",
      " [   0    0    0    0]]\n",
      "Fold 1 - Random Policy:\n",
      "  F1 Score: 0.1947\n",
      "  Confusion Matrix:\n",
      "[[1552 1819  835 1381]\n",
      " [3456 3415 3498 2934]\n",
      " [1481 1340  854  723]\n",
      " [   0    0    0    0]]\n",
      "Fold 1 - Strict Policy:\n",
      "  F1 Score: 0.1979\n",
      "  Confusion Matrix:\n",
      "[[1582 1821  616 1568]\n",
      " [3291 3564 3435 3013]\n",
      " [1187 1400  761 1050]\n",
      " [   0    0    0    0]]\n",
      "Processing fold 2/5\n",
      "Fold 2 - Generous Policy:\n",
      "  F1 Score: 0.2134\n",
      "  Confusion Matrix:\n",
      "[[3834 1419 1193 2366]\n",
      " [1314  818 1603  664]\n",
      " [2930 1954 1217 2673]\n",
      " [ 174  716   57  135]]\n",
      "Fold 2 - Random Policy:\n",
      "  F1 Score: 0.2356\n",
      "  Confusion Matrix:\n",
      "[[3599 1606 1011 2596]\n",
      " [ 994 1211 1164 1030]\n",
      " [3055 2329 1292 2098]\n",
      " [ 268  486  112  216]]\n",
      "Fold 2 - Strict Policy:\n",
      "  F1 Score: 0.2441\n",
      "  Confusion Matrix:\n",
      "[[3402 1780  889 2741]\n",
      " [ 788 1420  832 1359]\n",
      " [3039 2581 1289 1865]\n",
      " [ 324  330  156  272]]\n",
      "Processing fold 3/5\n",
      "Fold 3 - Generous Policy:\n",
      "  F1 Score: 0.1111\n",
      "  Confusion Matrix:\n",
      "[[ 764 1643 1210 1868]\n",
      " [5655 3589 3477 4764]\n",
      " [   0    0    0    0]\n",
      " [   0    0    0    0]]\n",
      "Fold 3 - Random Policy:\n",
      "  F1 Score: 0.1261\n",
      "  Confusion Matrix:\n",
      "[[ 831 1786 1206 1662]\n",
      " [5348 4258 3458 4421]\n",
      " [   0    0    0    0]\n",
      " [   0    0    0    0]]\n",
      "Fold 3 - Strict Policy:\n",
      "  F1 Score: 0.1352\n",
      "  Confusion Matrix:\n",
      "[[ 856 1883 1078 1668]\n",
      " [5187 4727 3457 4114]\n",
      " [   0    0    0    0]\n",
      " [   0    0    0    0]]\n",
      "Processing fold 4/5\n",
      "Fold 4 - Generous Policy:\n",
      "  F1 Score: 0.2271\n",
      "  Confusion Matrix:\n",
      "[[3060 3039 2848 1906]\n",
      " [1748 2486 1645 1651]\n",
      " [1637  770  968 1096]\n",
      " [   0    0    0    0]]\n",
      "Fold 4 - Random Policy:\n",
      "  F1 Score: 0.2085\n",
      "  Confusion Matrix:\n",
      "[[2999 3392 2094 2368]\n",
      " [1969 2170 1817 1574]\n",
      " [1326 1171  828 1146]\n",
      " [   0    0    0    0]]\n",
      "Fold 4 - Strict Policy:\n",
      "  F1 Score: 0.1958\n",
      "  Confusion Matrix:\n",
      "[[2982 3519 1671 2681]\n",
      " [2085 1949 1876 1620]\n",
      " [1122 1459  714 1176]\n",
      " [   0    0    0    0]]\n",
      "Processing fold 5/5\n",
      "Fold 5 - Generous Policy:\n",
      "  F1 Score: 0.1691\n",
      "  Confusion Matrix:\n",
      "[[1673 2507 1236 2201]\n",
      " [1454 1763  813 1484]\n",
      " [1763 3625  837 2357]\n",
      " [   0    0    0    0]]\n",
      "Fold 5 - Random Policy:\n",
      "  F1 Score: 0.1766\n",
      "  Confusion Matrix:\n",
      "[[1798 2367 1317 2135]\n",
      " [1481 1518  852 1663]\n",
      " [2028 3350 1130 2074]\n",
      " [   0    0    0    0]]\n",
      "Fold 5 - Strict Policy:\n",
      "  F1 Score: 0.1791\n",
      "  Confusion Matrix:\n",
      "[[1876 2296 1298 2147]\n",
      " [1536 1353  859 1766]\n",
      " [2218 3209 1290 1865]\n",
      " [   0    0    0    0]]\n",
      "\n",
      "Generous Policy - Cross-validation results:\n",
      "  Average F1 Score: 0.1825 ± 0.0408\n",
      "\n",
      "Random Policy - Cross-validation results:\n",
      "  Average F1 Score: 0.1883 ± 0.0366\n",
      "\n",
      "Strict Policy - Cross-validation results:\n",
      "  Average F1 Score: 0.1904 ± 0.0350\n"
     ]
    }
   ],
   "source": [
    "%cd /home/DAVIDSON/dutuller/Workspace/DRI1/MusicGen/\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from embeddings.h5_processor import H5DataProcessor, DatasetConfig\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "# Load dataset configurations\n",
    "with open(\"universal_music/NHS_full.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Process datasets to get all embeddings\n",
    "processor = H5DataProcessor(verbose=True)\n",
    "all_datasets = []\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_config in config['datasets']:\n",
    "    dataset = processor.process_h5_file(\n",
    "        processor.get_embedding_path(DatasetConfig(**dataset_config)),\n",
    "        DatasetConfig(**dataset_config)\n",
    "    )\n",
    "    all_datasets.append(dataset)\n",
    "\n",
    "# Combine datasets\n",
    "all_embeddings = np.vstack([d.embeddings for d in all_datasets])\n",
    "all_labels = [l for d in all_datasets for l in d.labels]\n",
    "all_filenames = [f for d in all_datasets for f in d.filenames]\n",
    "\n",
    "# Extract song IDs from filenames\n",
    "all_song_ids = []\n",
    "for filename in all_filenames:\n",
    "    match = re.search(r\"Discography-(\\d+)_\\d+.wav\", filename)\n",
    "    if match:\n",
    "        all_song_ids.append(int(match.group(1)))\n",
    "    else:\n",
    "        all_song_ids.append(None)\n",
    "\n",
    "# Extract unique song IDs (to ensure we split by song)\n",
    "unique_song_ids = np.unique([id for id in all_song_ids if id is not None])\n",
    "\n",
    "# Load human ratings\n",
    "df = pd.read_csv('universal_music/FFfull.csv', low_memory=False)\n",
    "web_df = df[df['study'] == 'web'].copy()\n",
    "\n",
    "# Load the ratings policy data\n",
    "web_df['generous'] = np.load(\"universal_music/web_survey_ratings_generous.npy\")\n",
    "web_df['random'] = np.load(\"universal_music/web_survey_ratings_random.npy\")\n",
    "web_df['strict'] = np.load(\"universal_music/web_survey_ratings_strict.npy\")\n",
    "\n",
    "# Now load the sample clip embeddings (what humans actually heard)\n",
    "with open(\"universal_music/NHS_samples.yaml\", 'r') as f:\n",
    "    samples_config = yaml.safe_load(f)\n",
    "\n",
    "dataset_config = DatasetConfig(**samples_config['datasets'][0])\n",
    "embedding_filename = processor.get_embedding_path(dataset_config)\n",
    "samples_dataset = processor.process_h5_file(embedding_filename, dataset_config)\n",
    "\n",
    "sample_filenames = samples_dataset.filenames\n",
    "sample_embeddings = samples_dataset.embeddings\n",
    "sample_song_ids = []\n",
    "\n",
    "for filename in sample_filenames:\n",
    "    match = re.search(r\"NAIV-(\\d+).wav\", filename)\n",
    "    if match:\n",
    "        sample_song_ids.append(int(match.group(1)))\n",
    "    else:\n",
    "        sample_song_ids.append(None)\n",
    "\n",
    "# Create unique label mapping\n",
    "unique_labels = sorted(set(all_labels))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Prepare for k-fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare results collection\n",
    "results = {\n",
    "    'generous': [],\n",
    "    'random': [], \n",
    "    'strict': []\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation on unique song IDs\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(unique_song_ids)):\n",
    "    print(f\"Processing fold {fold+1}/{k_folds}\")\n",
    "    \n",
    "    # Get song IDs for this fold\n",
    "    train_song_ids = unique_song_ids[train_idx]\n",
    "    test_song_ids = unique_song_ids[test_idx]\n",
    "    \n",
    "    # Get embeddings for training\n",
    "    train_mask = np.array([id in train_song_ids for id in all_song_ids])\n",
    "    X_train = all_embeddings[train_mask]\n",
    "    y_train = np.array([label_to_idx[label] for label in np.array(all_labels)[train_mask]])\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Train the model\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Process the test songs\n",
    "    test_sample_idx = np.array([i for i, id in enumerate(sample_song_ids) if id in test_song_ids])\n",
    "    \n",
    "    if len(test_sample_idx) == 0:\n",
    "        print(f\"No matching samples found for test songs in fold {fold+1}\")\n",
    "        continue\n",
    "        \n",
    "    X_test_sample = sample_embeddings[test_sample_idx]\n",
    "    test_sample_song_ids = np.array(sample_song_ids)[test_sample_idx]\n",
    "    \n",
    "    # Scale test data\n",
    "    X_test_scaled = scaler.transform(X_test_sample)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_sample = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Create mapping from song ID to prediction\n",
    "    id_to_pred = dict(zip(test_sample_song_ids, y_pred_sample))\n",
    "    \n",
    "    # Filter human ratings to only include test songs\n",
    "    fold_web_df = web_df[web_df['song'].isin(test_sample_song_ids)].copy()\n",
    "    \n",
    "    # Add model predictions\n",
    "    fold_web_df['model_pred'] = fold_web_df['song'].map(id_to_pred)\n",
    "    fold_web_df = fold_web_df.dropna(subset=['model_pred'])\n",
    "    \n",
    "    # Calculate metrics for each policy\n",
    "    for policy in ['generous', 'random', 'strict']:\n",
    "        cm = confusion_matrix(fold_web_df['model_pred'], fold_web_df[policy])\n",
    "        f1 = f1_score(fold_web_df['model_pred'], fold_web_df[policy], average='macro')\n",
    "        \n",
    "        results[policy].append({\n",
    "            'fold': fold + 1,\n",
    "            'confusion_matrix': cm,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold+1} - {policy.capitalize()} Policy:\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Calculate average results across folds\n",
    "for policy in results:\n",
    "    f1_scores = [r['f1_score'] for r in results[policy]]\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    print(f\"\\n{policy.capitalize()} Policy - Cross-validation results:\")\n",
    "    print(f\"  Average F1 Score: {avg_f1:.4f} ± {std_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4dd0a-2bee-4a83-8672-3c7c1ec1435f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
