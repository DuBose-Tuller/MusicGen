{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34adc423",
   "metadata": {},
   "source": [
    "## Old notebook\n",
    "\n",
    "This one was meant to use the model to predict ratings, but we are going to aggregate the human ratings instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ecb97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAVIDSON/dutuller/Workspace/DRI1/MusicGen/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/DAVIDSON/dutuller/Workspace/DRI1/MusicGen\n"
     ]
    }
   ],
   "source": [
    "%cd /home/DAVIDSON/dutuller/Workspace/DRI1/MusicGen/\n",
    "\n",
    "import numpy as np;\n",
    "from embeddings.h5_processor import H5DataProcessor, DatasetConfig, ProcessedDataset, AudioSegmentSplitter;\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd;\n",
    "from universal_music.models import RatingsClassifier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e11a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets using H5DataProcessor\n",
    "processor = H5DataProcessor()\n",
    "\n",
    "config = {\n",
    "    'dataset': 'embeddings/NHS',\n",
    "    'subfolder': 'samples/wav'\n",
    "}\n",
    "\n",
    "dataset = processor.process_h5_file(\n",
    "    processor.get_embedding_path(DatasetConfig(**config)),\n",
    "    DatasetConfig(**config)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12489d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = dataset.embeddings\n",
    "filenames = dataset.filenames\n",
    "\n",
    "df = pd.read_csv('universal_music/FFfull.csv', low_memory=False)\n",
    "\n",
    "web_df = df[df['study'] == 'web'].copy()\n",
    "field_df = df[df['study'] == 'field'].copy()\n",
    "\n",
    "song_functions = df.groupby(\"song\").agg({'songfunction': 'first'})\n",
    "\n",
    "# Get ratings and normalize\n",
    "web_ratings = web_df[[\"song\",\"danc\",\"heal\",\"baby\",\"love\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496b8dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ratings per song: 1094.82\n",
      "Min ratings per song: 919\n",
      "Max ratings per song: 1185\n",
      "Training songs: 94\n",
      "Test songs: 24\n",
      "X_train shape: (102698, 1536)\n",
      "y_train shape: (102698, 4)\n",
      "X_test shape: (26491, 1536)\n",
      "y_test shape: (26491, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to extract song ID from filename\n",
    "def extract_song_id(filename):\n",
    "    # Extract the base filename\n",
    "    base_name = filename.split('/')[-1]\n",
    "    # Extract numbers from filename\n",
    "    match = re.search(r'(\\d+)', base_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Extract song IDs from filenames\n",
    "song_ids = [extract_song_id(filename) for filename in filenames]\n",
    "song_id_to_idx = {song_id: idx for idx, song_id in enumerate(song_ids) if song_id is not None}\n",
    "\n",
    "# Get all ratings (each row is a rating by one participant for one song)\n",
    "ratings_df = web_df[[\"song\", \"danc\", \"heal\", \"baby\", \"love\"]]\n",
    "\n",
    "# Group by song to count ratings per song\n",
    "ratings_per_song = ratings_df.groupby('song').size()\n",
    "print(f\"Average ratings per song: {ratings_per_song.mean():.2f}\")\n",
    "print(f\"Min ratings per song: {ratings_per_song.min()}\")\n",
    "print(f\"Max ratings per song: {ratings_per_song.max()}\")\n",
    "\n",
    "# Get unique song IDs that have embeddings\n",
    "unique_song_ids = list(set(song_id_to_idx.keys()).intersection(set(ratings_df['song'])))\n",
    "\n",
    "# Split song IDs into train and test sets\n",
    "train_song_ids, test_song_ids = train_test_split(unique_song_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training songs: {len(train_song_ids)}\")\n",
    "print(f\"Test songs: {len(test_song_ids)}\")\n",
    "\n",
    "# Create X and y matrices for training and testing\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "# For each rating entry\n",
    "for _, row in ratings_df.iterrows():\n",
    "    song_id = row['song']\n",
    "    \n",
    "    # Check if we have an embedding for this song\n",
    "    if song_id in song_id_to_idx:\n",
    "        embedding_idx = song_id_to_idx[song_id]\n",
    "        embedding = embeddings[embedding_idx]\n",
    "        \n",
    "        # Create rating array\n",
    "        rating = [\n",
    "            row['danc'],\n",
    "            row['heal'],\n",
    "            row['baby'],\n",
    "            row['love']\n",
    "        ]\n",
    "        \n",
    "        # Add to appropriate set based on song ID\n",
    "        if song_id in train_song_ids:\n",
    "            X_train_list.append(embedding)\n",
    "            y_train_list.append(rating)\n",
    "        elif song_id in test_song_ids:\n",
    "            X_test_list.append(embedding)\n",
    "            y_test_list.append(rating)\n",
    "\n",
    "# Convert lists to arrays\n",
    "X_train = np.array(X_train_list)\n",
    "y_train = np.array(y_train_list)\n",
    "X_test = np.array(X_test_list)\n",
    "y_test = np.array(y_test_list)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0722d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create and configure model\n",
    "model = RatingsClassifier(max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19b55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb43df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5.5613: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<universal_music.models.RatingsClassifier at 0x154ceffc3f10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(X_train_scaled, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d90630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting ratings on the test set\n",
    "pred_ratings = model.predict_ratings(X_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
